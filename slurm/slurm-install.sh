#!/bin/sh

# Author: Cole McKnight
# Email: cbmckni@clemson.edu
# Organization: Clemson University HPC
# Github: https://github.com/cuhpc 
# Last Updated: 2/24/19
# Description: CentOS Slurm Installation. 

# NOTE: Many commands need sudo permission. Run script as root or with sudo. Or just run each command one by one with sudo.
# NOTE: DO NOT RUN AS SCRIPT!! You must manually generate multiple .conf files, and the defaults I provide will probably not work.

# Prereqs: (MUST be done prior on respective nodes)

# pi-prep.sh
# centos-nfs-server.sh
# centos-nfs-client.sh
# slurm-base.sh
# slurm-db.sh
# slurm-compute.sh

# Install Guide: https://www.slothparadise.com/how-to-install-slurm-on-centos-7-cluster/

# Variables:

export SLURM_VERSION=18.08.5-2 \
  && export MUNGE_UID=981 \
  && export SLURM_UID=982 \
  && export ARCH=armv7hl \ # system architecture (ex. ARM for RaspPi)
  && export COMPUTENODES=('130.127.248.160' '130.127.248.161' '130.127.248.162' '130.127.248.163' '130.127.248.164' '130.127.248.165' '130.127.248.166' '130.127.248.167') \ # list of all node IPs
  && export NODEHOSTS=('pi0' 'pi1' 'pi2' 'pi3' 'pi4' 'pi5' 'pi6' 'pi7') \ # list of all node hostnames in same order
  && export ROOTPASS=clemsontigers \ #root password for all nodes (be careful! delete after you are done)
  && export DBNODE=130.127.248.161 \ # Database node hostname
  && export HEADNODE=130.127.248.160 # Head node hostname


# Add hostnames to /etc/hosts for each node
SIZE=${#NODEHOSTS[@]}
for i in ${COMPUTENODES[@]};
do
  for j in $(seq 0 "$(($SIZE-1))"); 
  do
    sshpass -p $ROOTPASS ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@"$i" \
      'echo '"${COMPUTENODES[$j]} ${NODEHOSTS[$j]}"' >> /etc/hosts'
  done
  echo "$i Hosts registered."
done

# Download Slurm in shared folder. NOTE: Run this on database node for faster build (avoid NFS latency). 

cd /var/nfs-slurm # nfs-slurm is shared dir created during NFS setup. Will be /mnt/nfs/var/nfs-slurm if not on DB node
wget https://download.schedmd.com/slurm/slurm-$SLURM_VERSION.tar.bz2

# Install Slurm. 

# Install dependencies

yum -y groupinstall "Development Tools"
yum -y install rpm-build \
               yum-utils \
               readline-devel \
               openssl-devel \
               openssl \
               perl-devel \
               pam-devel \
               mysql-devel \
               sshpass 

# Build RPMs

rpmbuild -ta slurm-$SLURM_VERSION.tar.bz2 
libtool --finish /usr/lib/slurm
libtool --finish /lib/security

# Copy RPMs to shared folder

cd /root/rpmbuild/RPMS/$ARCH
mkdir /mnt/nfs/var/nfs-slurm/slurm-rpms
cp slurm* /mnt/nfs/var/nfs-slurm/slurm-rpms

# ssh into each node and install Slurm

for i in "${COMPUTENODES[@]}"; 
do
  if [ "$i" == "$DBNODE" ]
  then
    sshpass -p $ROOTPASS ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@"$i" \
      'yum -y --nogpgcheck localinstall /var/nfs-slurm/slurm-rpms/slurm*'
  else
    sshpass -p $ROOTPASS ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@"$i" \
      'yum -y --nogpgcheck localinstall /mnt/nfs/var/nfs-slurm/slurm-rpms/slurm*'
  fi
  echo "$i Slurm installed."
done

# IMPORTANT: Go to https://slurm.schedmd.com/configurator.easy.html and generate slurm.conf. 

# Example slurm.conf:

# Save as /etc/slurm/slurm.conf. PASTE ENTIRE slurm.conf HERE

mkdir -p /etc/slurm
cat > /etc/slurm/slurm.conf <<EOF
# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
SlurmctldHost=pi0
# 
MailProg=/bin/mail 
MpiDefault=none
#MpiParams=ports=#-# 
ProctrackType=proctrack/cgroup
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
#SlurmctldPort=6817 
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818 
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
#SlurmdUser=root 
StateSaveLocation=/var/spool/slurmctld
SwitchType=switch/none
TaskPlugin=task/affinity
# 
# 
# TIMERS 
#KillWait=30 
#MinJobAge=300 
#SlurmctldTimeout=120 
#SlurmdTimeout=300 
# 
# 
# SCHEDULING 
FastSchedule=1
SchedulerType=sched/backfill
SelectType=select/cons_res
SelectTypeParameters=CR_Core
# 
# 
# LOGGING AND ACCOUNTING 
AccountingStorageType=accounting_storage/none
ClusterName=HAL9000
#JobAcctGatherFrequency=30 
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=info 
SlurmctldLogFile=/var/log/slurmctld.log
#SlurmdDebug=info 
SlurmdLogFile=/var/log/slurmd.log
# 
# 
# COMPUTE NODES 
NodeName=pi[0-7] CPUs=4 State=UNKNOWN 
PartitionName=main Nodes=pi[0-7] Default=YES MaxTime=INFINITE State=UP
EOF

# Copy slurm.conf to other nodes
cd /etc/slurm
for i in "${COMPUTENODES[@]}"; 
do
  
  echo "$i slurm.conf copied."
done

# Create slurmdbd.conf. Save to /etc/slurm/slurmdbd.conf

# Example slurmdbd.conf

cat > /etc/slurm/slurmdbd.conf <<EOF
#
# Example slurmdbd.conf file.
#
# See the slurmdbd.conf man page for more information.
#
# Archive info
#ArchiveJobs=yes
#ArchiveDir="/tmp"
#ArchiveSteps=yes
#ArchiveScript=
#JobPurge=12
#StepPurge=1
#
# Authentication info
AuthType=auth/munge
AuthInfo=/var/run/munge/munge.socket.2
#
# slurmDBD info
DbdAddr=$DBNODE
DbdHost=pi1
DbdPort=6819
SlurmUser=slurm
#MessageTimeout=300
DebugLevel=4
#DefaultQOS=normal,standby
LogFile=/var/log/slurm/slurmdbd.log
PidFile=/var/run/slurmdbd.pid
#PluginDir=/usr/lib/slurm
#PrivateData=accounts,users,usage,jobs
#TrackWCKey=yes
#
# Database info
StorageType=accounting_storage/mysql
StorageHost=pi1
StoragePort=3306
StoragePass=clemsontigers
StorageUser=slurm
StorageLoc=slurm_acct_db
EOF

# Create cgroup.conf and cgroup_allowed_devices_file.conf files and put them in /etc/slurm 

# Guide: https://bugs.schedmd.com/show_bug.cgi?id=3701

# Example cgroup.conf:

# Save as /etc/slurm/cgroup.conf. PASTE ENTIRE cgroup.conf HERE

cat > /etc/slurm/cgroup.conf <<EOF
CgroupMountpoint="/sys/fs/cgroup"
CgroupAutomount=yes
CgroupReleaseAgentDir="/etc/slurm/cgroup"
AllowedDevicesFile="/etc/slurm/cgroup_allowed_devices_file.conf"
ConstrainCores=no
TaskAffinity=no
ConstrainRAMSpace=yes
ConstrainSwapSpace=no
ConstrainDevices=no
AllowedRamSpace=100
AllowedSwapSpace=0
MaxRAMPercent=100
MaxSwapPercent=100
MinRAMSpace=30
EOF

# Example cgroup_allowed_devices_file.conf:

# Save as /etc/slurm/cgroup_allowed_devices_file.conf. PASTE ENTIRE cgroup_allowed_devices_file.conf HERE

cat > /etc/slurm/cgroup_allowed_devices_file.conf <<EOF
/dev/null
/dev/urandom
/dev/zero
/dev/sda*
/dev/cpu/*/*
/dev/pts/*
EOF

# Copy files to each node

cd /etc/slurm
for i in "${COMPUTENODES[@]}"; 
do
  sshpass -p $ROOTPASS scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -r slurm.conf root@"$i":/etc/slurm/slurm.conf
  sshpass -p $ROOTPASS scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -r slurmdbd.conf root@"$i":/etc/slurm/slurmdbd.conf
  sshpass -p $ROOTPASS scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -r cgroup.conf root@"$i":/etc/slurm/cgroup.conf
  sshpass -p $ROOTPASS scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -r cgroup_allowed_devices_file.conf root@"$i":/etc/slurm/cgroup_allowed_devices_file.conf
  echo "$i .conf files copied."
done


